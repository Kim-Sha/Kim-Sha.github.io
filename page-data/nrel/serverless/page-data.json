{
    "componentChunkName": "component---src-templates-blog-js",
    "path": "/nrel/serverless/",
    "result": {"data":{"site":{"siteMetadata":{"title":"Kim Sha - Data Scientist"}},"mdx":{"fields":{"slug":"/nrel/serverless/"},"id":"41167f18-aa9f-51a2-bfae-6c5d159124a7","excerpt":"Last time out , I built an ETL pipeline\non top of the Scrapy framework to gather data from NREL's station in Golden, CO.\nAs much as I'd enjoy dedicating myâ€¦","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Serverless Web Scraping with AWS, Part II\",\n  \"date\": \"2021-02-21\",\n  \"description\": \"ETL & Analysis of Solar Irradiance Data, Part 2\",\n  \"tags\": [\"Web Scraping\", \"ETL\", \"NREL\"],\n  \"featured\": \"severless_schedule.png\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar LazyPlot = makeShortcode(\"LazyPlot\");\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", {\n    className: \"lead\"\n  }, \"How do I build an automated ETL pipeline from a simple web scraper? Over two posts, I document the resources I leverage to periodically extract solar irradiance and meteorological data from the Solar Energy Research Institute's (SRRL) online portal. This second post will focus on automating the ETL pipeline.\"), mdx(\"figure\", null, mdx(LazyPlot, _extends({}, irradiance_animate_south40, {\n    mdxType: \"LazyPlot\"\n  })), mdx(\"figcaption\", null, \"A taste of the measurements made by the NREL Baseline Measurement System in Golden, Colorado. This is equivalent to the solar resource for collectors with fixed-tilt and optimized for year-round performance.\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://kim-sha.github.io/nrel/scraper/\"\n  }, \"Last time out\"), \", I built an ETL pipeline\\non top of the Scrapy framework to gather data from NREL's station in Golden, CO.\\nAs much as I'd enjoy dedicating my remaining days to manually triggering this web\\nscraper, I can't let it conflict with my Netflix schedule. Fortunately, I'm told\\nsome guy built a slew of somewhat profitable services for this exact reason.\\nHis name is Jeff.\"), mdx(\"h2\", null, \"Core Tools\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"590px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/0ff7f8bbed78dc0358a7a8b67c364752/a7707/nrel_scraper_banner.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"31.08108108108108%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAACdfAAAnXwEdhrpqAAABeElEQVQY002Ry27TQBiF/R5IFUhITZpbPZ5x8GXs0rgkMYkTj9OL2giUFNGiXihFICQv2gU7BKx4DHgHtrzUh2KKYHF0dH79Z3H0WY7OkYFBaYMTGKQ2qKhARjNsXSD0rMpuXOCEhqZXIHVe3aq/cIL0nuL446prydBg72S0shQ5mtJ2x9TFgEbbxe0o7KagIbaoiZSunnA0GSHCgoZMaakhnd5z7KNPeNMrhJ9hiXiCP9/jYO8lW/mA0eKMk/Mb1tbucyE6FLWH1GXM2/ILUbLLz9LnIO2ze1zybPmGezvXbN784vGHH9hehtXRY4anC84uSzbyhNmrKy6uP7Je3yDpKrTdxg37vLv9xqGZ8fV0QDlPWJ7fsjx5z4P4GPH6O/GLz9je+G7yk4xWPkTmfybXRB+hNM6jCOEGbKoe6/YQNyroJnO83iFNldJQKWFvn+3RAp3sI4IplhP+B8X/B0WEBhHklRxt7qDkyHBa+V8oK1CrvPIVlN+RYtuvPzPV6AAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Tools and process of web scraping NREL data\",\n    \"title\": \"Tools and process of web scraping NREL data\",\n    \"src\": \"/static/0ff7f8bbed78dc0358a7a8b67c364752/fcda8/nrel_scraper_banner.png\",\n    \"srcSet\": [\"/static/0ff7f8bbed78dc0358a7a8b67c364752/12f09/nrel_scraper_banner.png 148w\", \"/static/0ff7f8bbed78dc0358a7a8b67c364752/e4a3f/nrel_scraper_banner.png 295w\", \"/static/0ff7f8bbed78dc0358a7a8b67c364752/fcda8/nrel_scraper_banner.png 590w\", \"/static/0ff7f8bbed78dc0358a7a8b67c364752/efc66/nrel_scraper_banner.png 885w\", \"/static/0ff7f8bbed78dc0358a7a8b67c364752/c83ae/nrel_scraper_banner.png 1180w\", \"/static/0ff7f8bbed78dc0358a7a8b67c364752/a7707/nrel_scraper_banner.png 3850w\"],\n    \"sizes\": \"(max-width: 590px) 100vw, 590px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"p\", null, \"I've already been using one of Jeff's 'somewhat profitable' offerings -\\nAmazon RDS - to host my Postgres instance. This time round, I'll be\\nleveraging \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\"\n  }, \"AWS Lambda\"), \",\\na compute service that allows me to farm off code that can be triggered in response\\nto manual invocations, network requests, and events. We'll be running the scraper\\nas a scheduled event, i.e., a cron job.\"), mdx(\"p\", null, \"This is where \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.serverless.com/framework/docs/providers/aws/guide/intro/\"\n  }, \"Serverless\"), \"\\ncomes in. As a framework, it handles much of the infrastructure required to\\nbuild and deploy an application using\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.serverless.com/blog/serverless-architecture-code-patterns\"\n  }, \"serverless code patterns\"), \".\\nMy use-case will be fairly simple: deploy 1 service to AWS Lambda comprised of\\n1 function that's triggered daily.\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"If you haven't done so already, set up an\\n\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc\"\n  }, \"AWS account\"), \".\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Install the Serverless framework with \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"npm install -g serverless\"), \" (requires Node.js and NPM)\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Install \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.docker.com/\"\n  }, \"Docker\"), \", which will be used to bundle\\npython dependencies for deployment.\")), mdx(\"h2\", null, \"Configuring Serverless Access to AWS\"), mdx(\"p\", null, \"In the root of the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"nrel-scraper\"), \" project folder (containing \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"scrapy.cfg\"), \"), we\\ninitialize and name a service from a template:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-shell\"\n  }, \"$ serverless create --template aws-python3 --name nrel-scraper\\n\")), mdx(\"p\", null, \"This will hook us up with a series of \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.serverless.com/framework/docs/providers/aws/guide/serverless.yml/\"\n  }, \"configurations\"), \"\\nprovided in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"serverless.yml\"), \", a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \".gitignore\"), \", and a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"handler.py\"), \" script (more on that later).\"), mdx(\"p\", null, \"Next, you'll want to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.youtube.com/watch?v=KngM5bfpttA\"\n  }, \"generate some AWS access keys\"), \"\\nfor Serverless latch on to. If you're feeling the need for speed, you\\ncan dump them into a gitignored dotenv file and export them as environment variables\\nin your shell (\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"source .env\"), \") before deployment. I prefer the more permanent\\nsolution of setting up AWS profiles under \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"~/.aws/credentials\"), \":\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-ini\"\n  }, \"[serverless-admin]\\naws_access_key_id = ********************\\naws_secret_access_key = ********************\\n\")), mdx(\"p\", null, \"There are several ways to invoke different profiles during deployment, but for\\nmy purposes it's sufficient to just edit the provider field in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"serverless.yml\"), \":\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"provider:\\n  name: aws\\n  runtime: python3.7\\n  lambdaHashingVersion: 20201221\\n  profile: serverless-admin\\n\")), mdx(\"h2\", null, \"Packaging the Function\"), mdx(\"p\", null, \"Up to this point, we've been manually invoking the scraper with \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"scrapy crawl midc\"), \".\\nJeff wants us to package this into a neat function that handles \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"event\"), \" and\\n\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"context\"), \" objects that are passed to it. You can read more about these arguments\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.aws.amazon.com/lambda/latest/dg/python-handler.html\"\n  }, \"here\"), \".\"), mdx(\"p\", null, \"Recall that intializing Serverless provided us a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"handler.py\"), \". Since I'm not\\npassing any \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"event\"), \" data for the Lambda function to process, my handler function\\nis excruciatingly boring:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"# ./handler.py\\n\\nimport json\\nimport sys\\nfrom nrel_scraper.crawl import crawl\\n\\ndef scrape(event={}, context={}):\\n    crawl()\\n\\nif __name__ == \\\"__main__\\\":\\n    scrape(event)\\n\")), mdx(\"p\", null, \"The imported \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"crawl\"), \" method just replicates the manual invocation of the scaper\\nby using \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.scrapy.org/en/latest/topics/practices.html\"\n  }, \"Scrapy API\"), \"\\n(pardon the hacky \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://stackoverflow.com/questions/52291998/unable-to-get-results-from-scrapy-on-aws-lambda\"\n  }, \"sqlite3 hotfix\"), \"\\nI had to dirty my hands with):\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"# ./nrel_scraper/crawl.py\\n\\nimport scrapy\\nfrom scrapy.spiderloader import SpiderLoader\\nfrom scrapy.crawler import CrawlerProcess\\nfrom scrapy.utils.project import get_project_settings\\n\\n# Start sqlite3 fix\\nimport imp\\nimport sys\\nsys.modules[\\\"sqlite\\\"] = imp.new_module(\\\"sqlite\\\")\\nsys.modules[\\\"sqlite3.dbapi2\\\"] = imp.new_module(\\\"sqlite.dbapi2\\\")\\n# End sqlite3 fix\\n\\ndef crawl(spider_name=\\\"midc\\\", spider_kwargs={}):\\n\\n    project_settings = get_project_settings()\\n    spider_loader = SpiderLoader(project_settings)\\n    spider_cls = spider_loader.load(spider_name)\\n\\n    process = CrawlerProcess(project_settings)\\n    process.crawl(spider_cls, **spider_kwargs)\\n    process.start()\\n\")), mdx(\"h2\", null, \"Managing Dependencies\"), mdx(\"p\", null, \"Don't forget the never-ending joy of \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.serverless.com/blog/serverless-python-packaging\"\n  }, \"bundling dependencies\"), \"\\nso that they'll be provisioned 'in the cloud'. I'll use pipenv to spawn a virtual\\nPython 3 environment:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-shell\"\n  }, \"$ pipenv --three\\n$ pipenv shell\\n$ pipenv install <for_each_package>\\n$ pipenv lock\\n\")), mdx(\"p\", null, \"The required packages for \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"nrel-scraper\"), \" are Scrapy, sqlalchemy, pandas,\\npython-dotenv, and psycopg2-binary. If you ever needed proof of\\nhow over-engineered this simple scraper is, that list of bloated\\npackages should do the trick.\"), mdx(\"p\", null, \"Serverless will need to bundle these dependencies specified in the newly\\ngenerated \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Pipfile\"), \". Fortunately, there's a handy \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.serverless.com/plugins/serverless-python-requirements\"\n  }, \"plugin\"), \"\\nfor this purpose:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-shell\"\n  }, \"$ serverless plugin install -n serverless-python-requirements\\n\")), mdx(\"p\", null, \"The plugin will be automatically added to the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"package.json\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"serverless.yml\"), \".\\nWe'll also need to add a custom field to the latter enabling the usage of Docker\\nto compile packages that aren't native to Python:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"plugins:\\n  - serverless-python-requirements\\ncustom:\\n  pythonRequirements:\\n    slim: true # Omits tests, __pycache__, *.pyc etc\\n    usePipenv: true\\n    dockerizePip: non-linux\\n\")), mdx(\"h2\", null, \"Serverless Deployment\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"590px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/a74c77378b323ac393b9d191c36a57f7/67a79/severless_schedule.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"60.13513513513513%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAAB90lEQVQoz42TT28SYRDG+TIGjZFC6S4L7ALLLoFCF3aXFWhaU6o0IrSpEmvUeJAY26YXNcZo/HtpPKg3L3po4ufQj6DpV/iZXWC7pRx6mMw78848M++8z4QWFJu4bOFqV+bTpn8O+s8roWCiq9Nak2iyypxkIGRGvtG9q21PC8ppeyRjwAnYnFSlUO2wfW+X4f4rKs5NIlIVwSs06WAEcjlZJ5a2p0DHgEKmTiSxhHPtNot21+uusdqn1uojpA3iio0bI2ad0VmxWLfKaHqN6AzQkBt4MVbkweNndAdPuesoHN2X+LEj8XWQQ1FNLsXLhKNFosmal3T0UObXI9nrVPSfbJ3MMJIwMK52+fztJ8N2kfebeT5saXzcUlFyNdr9IS9eH1Iw2oQFk+3WIsfPBa5bFS6IDmLG8sfizzCWMpH1ZQ46Bd71crzp5fkyyCJnDV5++s7ff8fU13Yw9AK/95O87an82UugajV/nn6HEyOaMuk3yux1SjxZL7F7o4Ss2kTT7u+3CIsmm80KBxsl5hWHwzs6jSWDKyl7/PM+oMs/i5TWRK9tUKz3SBfXELVVMsUVn59x2SSpr6Bbt7znq2YPKb9MPMDd0DSJE9k6YmZcUTZZCJA7rgR5aJPw4k4vwBlinxDZPrMxvj8QN30XOu+azSo0K/c/HnfAWPU5KgIAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"AWS Lambda functions can be triggered on a schedule\",\n    \"title\": \"AWS Lambda functions can be triggered on a schedule\",\n    \"src\": \"/static/a74c77378b323ac393b9d191c36a57f7/fcda8/severless_schedule.png\",\n    \"srcSet\": [\"/static/a74c77378b323ac393b9d191c36a57f7/12f09/severless_schedule.png 148w\", \"/static/a74c77378b323ac393b9d191c36a57f7/e4a3f/severless_schedule.png 295w\", \"/static/a74c77378b323ac393b9d191c36a57f7/fcda8/severless_schedule.png 590w\", \"/static/a74c77378b323ac393b9d191c36a57f7/efc66/severless_schedule.png 885w\", \"/static/a74c77378b323ac393b9d191c36a57f7/c83ae/severless_schedule.png 1180w\", \"/static/a74c77378b323ac393b9d191c36a57f7/67a79/severless_schedule.png 1408w\"],\n    \"sizes\": \"(max-width: 590px) 100vw, 590px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"p\", null, \"All that's left is to rehaul the functions field in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"serverless.yml\"), \" to schedule\\nthe cron job and point to the correct handler method. Note that all\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html#CronExpressions\"\n  }, \"cron expressions\"), \"\\nshould be passed in UTC:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yml\"\n  }, \"functions:\\n  nrelScrape:\\n    # handler method for nrel-scraper\\n    handler: handler.scrape\\n    # Seconds until function times out\\n    timeout: 30 \\n    events:\\n      # Definition of the cron job that's passed to AWS CloudWatch\\n      - schedule:\\n          name: nrel-midc-daily-ingestion\\n          description: Scheduled daily scraping of NREL MIDC at 11:59PM MST (UTC-7:00)\\n          rate: cron(59 6 * * ? *)\\n\")), mdx(\"p\", null, \"This wraps up our \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"serverless.yml\"), \". You can learn more from this\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.serverless.com/framework/docs/providers/aws/guide/serverless.yml/\"\n  }, \"example reference\"), \"\\nor check out the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/Kim-Sha/nrel-scraper\"\n  }, \"GitHub repo\"), \" for this project.\\nBut first, deploy!\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-shell\"\n  }, \"$ serverless deploy --verbose\\n\")), mdx(\"p\", null, \"If by some miracle, the permissions across Amazon RDS, Lambda, and Serverless\\nare all perfectly aligned, c'est fini. Personally, I had to do some\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-rds-connection-timeouts/\"\n  }, \"troubleshooting\"), \"\\nto grease the wheels.\"), mdx(\"p\", null, \"The function will now self-trigger according to the schedule dictated to\\nCloudWatch. You can still manually trigger the job to run on Lambda:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-shell\"\n  }, \"$ serverless invoke -f nrelScrape --log\\n\")), mdx(\"p\", null, \"And we're done here! If you've somehow made it through this slog, I must both\\napologize and thank you for bearing with me. Now, we sit back and let the solar\\nirradiance and meteorological data from the finely calibrated pyranometers\\nand pyrheliometers of the Solar Energy Research Institute's Baseline Measurement\\nSystem roll in...\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Serverless Web Scraping with AWS, Part II","date":"February 21, 2021","description":"ETL & Analysis of Solar Irradiance Data, Part 2","image":{"childImageSharp":{"resize":{"src":"/static/a74c77378b323ac393b9d191c36a57f7/f3583/severless_schedule.png","height":718,"width":1200}}}}},"previous":{"fields":{"slug":"/nrel/scraper/"},"frontmatter":{"title":"Serverless Web Scraping with AWS, Part I"}},"next":{"fields":{"slug":"/clean-energy-innovators/goran-bolin/"},"frontmatter":{"title":"GÃ¶ran Bolin - Founder, SaltX Technology"}}},"pageContext":{"id":"41167f18-aa9f-51a2-bfae-6c5d159124a7","slug":"/nrel/serverless/","previousPostId":"a573498e-91ef-59e6-977d-937f10e7621e","nextPostId":"6d52505c-bb3c-5c5c-858a-bdc3de2d9ce0"}},
    "staticQueryHashes": ["2583974970","3940526654"]}