{
    "componentChunkName": "component---src-templates-blog-js",
    "path": "/nrel/scraper/",
    "result": {"data":{"site":{"siteMetadata":{"title":"Kim Sha - Data Scientist"}},"mdx":{"fields":{"slug":"/nrel/scraper/"},"id":"70fdd9c8-02bd-593f-bfe3-22ea657a5303","excerpt":"The National Renewable Energy Laboratory (NREL) established the SRRL as a means\nto monitor solar resources on a continuous basis. Located in Golden, CO, the labâ€¦","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Serverless Web Scraping with AWS, Part I\",\n  \"date\": \"2021-01-31\",\n  \"description\": \"ETL & Analysis of Solar Irradiance Data, Part 1\",\n  \"tags\": [\"Web Scraping\", \"ETL\", \"NREL\"],\n  \"featured\": \"nrel_scraper_process.png\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", {\n    className: \"lead\"\n  }, \"How do I build an automated ETL pipeline from a simple web scraper? Over two posts, I document the resources I leverage to periodically extract solar irradiance and meteorological data from the Solar Energy Research Institute's (SRRL) online portal. This first post will focus on developing the ETL pipeline itself.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"590px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/0ff7f8bbed78dc0358a7a8b67c364752/a7707/nrel_scraper_banner.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"31.08108108108108%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAACdfAAAnXwEdhrpqAAABeElEQVQY002Ry27TQBiF/R5IFUhITZpbPZ5x8GXs0rgkMYkTj9OL2giUFNGiXihFICQv2gU7BKx4DHgHtrzUh2KKYHF0dH79Z3H0WY7OkYFBaYMTGKQ2qKhARjNsXSD0rMpuXOCEhqZXIHVe3aq/cIL0nuL446prydBg72S0shQ5mtJ2x9TFgEbbxe0o7KagIbaoiZSunnA0GSHCgoZMaakhnd5z7KNPeNMrhJ9hiXiCP9/jYO8lW/mA0eKMk/Mb1tbucyE6FLWH1GXM2/ILUbLLz9LnIO2ze1zybPmGezvXbN784vGHH9hehtXRY4anC84uSzbyhNmrKy6uP7Je3yDpKrTdxg37vLv9xqGZ8fV0QDlPWJ7fsjx5z4P4GPH6O/GLz9je+G7yk4xWPkTmfybXRB+hNM6jCOEGbKoe6/YQNyroJnO83iFNldJQKWFvn+3RAp3sI4IplhP+B8X/B0WEBhHklRxt7qDkyHBa+V8oK1CrvPIVlN+RYtuvPzPV6AAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Tools and process of web scraping NREL data\",\n    \"title\": \"Tools and process of web scraping NREL data\",\n    \"src\": \"/static/0ff7f8bbed78dc0358a7a8b67c364752/fcda8/nrel_scraper_banner.png\",\n    \"srcSet\": [\"/static/0ff7f8bbed78dc0358a7a8b67c364752/12f09/nrel_scraper_banner.png 148w\", \"/static/0ff7f8bbed78dc0358a7a8b67c364752/e4a3f/nrel_scraper_banner.png 295w\", \"/static/0ff7f8bbed78dc0358a7a8b67c364752/fcda8/nrel_scraper_banner.png 590w\", \"/static/0ff7f8bbed78dc0358a7a8b67c364752/efc66/nrel_scraper_banner.png 885w\", \"/static/0ff7f8bbed78dc0358a7a8b67c364752/c83ae/nrel_scraper_banner.png 1180w\", \"/static/0ff7f8bbed78dc0358a7a8b67c364752/a7707/nrel_scraper_banner.png 3850w\"],\n    \"sizes\": \"(max-width: 590px) 100vw, 590px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"p\", null, \"The National Renewable Energy Laboratory (NREL) established the SRRL as a means\\nto monitor solar resources on a continuous basis. Located in Golden, CO, the lab\\nhouses a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://midcdmz.nrel.gov/apps/sitehome.pl?site=BMS\"\n  }, \"Baseline Measurement System (BMS)\"), \"\\nthat records a trove of irradiance data at a granularity of 1-minute intervals.\"), mdx(\"p\", null, \"While the website contains some fairly user-friendly endpoints, download options\\naren't updated on a real-time basis, and the live views refresh periodically.\\nThus, I decided to construct an \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/Kim-Sha/nrel-scraper\"\n  }, \"ETL pipeline\"), \"\\nto access it, just so I can do some EDA down the line. Textbook overkill.\"), mdx(\"h2\", null, \"Setting up a PostgreSQL Instance on RDS\"), mdx(\"p\", null, \"Turns out there're better ways to store data than etching it into the walls of my\\nrental home, so I set up a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://aws.amazon.com/getting-started/tutorials/create-connect-postgresql-db/\"\n  }, \"Postgres database on Amazon Web Services\"), \".\\nIt's a simple setup with 3 tables:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"stations\"), \": currently just links extracted data to one measurement site:\\nNREL Solar Radiation Research Laboratory (BMS). If we pull data from other\\nsites in the future, this table will become more relevant.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"irradiance\"), \": tracks global, direct, and diffuse irradiance in W/m^2 using\\ndifferent instruments. There's even vertical-facing collectors logging data\\nfor each of the cardinal directions.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"meteorological\"), \": tracks hyper-local meteorological conditions, such as\\nalbedo, zenith / azimuth angles relative to the sun, temperature, and wind speed.\")), mdx(\"p\", null, \"The Measurement and Instrumentation Data Center (MIDC) has\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://midcdmz.nrel.gov/apps/html.pl?site=BMS;page=instruments#G40S22\"\n  }, \"descriptions of metrics and the instruments\"), \"\\nresponsible for recording them. Check out \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/Kim-Sha/nrel-scraper\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"./query_history\")), \"\\nin the repo to see the which fields I'm tracking.\"), mdx(\"h2\", null, \"Creating the Web Scraper\"), mdx(\"p\", null, \"Now that we have a database to load into, we can focus on extracting and\\ntransforming data using Python. We'll rely on the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://doc.scrapy.org/en/latest/intro/overview.html\"\n  }, \"Scrapy framework\"), \"\\nhere to crawl and extract structured data from the target endpoints. Running\\n\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"scrapy startproject nrel_scraper\"), \" provides a simple architecture to develop on. \"), mdx(\"h3\", null, \"MIDC Spider\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"# ./nrel_scraper/spiders/midc_spider.py\\n\\nimport scrapy\\nfrom nrel_scraper.items import NrelScraperItem\\n\\nclass MidcSpider(scrapy.Spider):\\n\\n    name = \\\"midc\\\"\\n    start_urls = [\\n        'https://midcdmz.nrel.gov/apps/plot.pl?site=BMS&start=20200101&live=1&zenloc=222&amsloc=224&time=1&inst=15&inst=22&inst=41&inst=43&inst=45&inst=46&inst=47&inst=48&inst=49&inst=50&inst=51&inst=52&inst=53&inst=55&inst=62&inst=63&inst=74&inst=75&type=data&wrlevel=6&preset=0&first=3&math=0&second=-1&value=0.0&global=-1&direct=-1&diffuse=-1&user=0&axis=1',\\n        'https://midcdmz.nrel.gov/apps/plot.pl?site=BMS&start=20200101&live=1&zenloc=222&amsloc=224&time=1&inst=130&inst=131&inst=132&inst=134&inst=135&inst=142&inst=143&inst=146&inst=147&inst=148&inst=149&inst=150&inst=151&inst=156&inst=157&inst=158&inst=159&inst=160&inst=161&inst=164&inst=172&inst=173&type=data&wrlevel=6&preset=0&first=3&math=0&second=-1&value=0.0&global=-1&direct=-1&diffuse=-1&user=0&axis=1'\\n        ]\\n\\n    # Associate url with corresponding postgres table in RDS instance\\n    map_table = {k:v for k, v in zip(start_urls, ['irradiance', 'meteorological'])}\\n\\n    def parse(self, response):\\n\\n        data_text = response.xpath('//body/p/text()').get()\\n\\n        # Identify correct table to push parsed data to\\n        push_to_table = self.map_table.get(response.request.url, None)\\n\\n        # Yield data\\n        yield NrelScraperItem(data_text = data_text, table = push_to_table)\\n\")), mdx(\"p\", null, \"The actual spider itself has a fairly simple definition, with \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"start_urls\"), \" listing\\nthe endpoints that Scrapy should crawl (irradiance and meteorological data,\\nrespectively). Because the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"parse()\"), \" method is Scrapy's default callback method\\nfor each of the requests, it doesn't need to be called explicitly.\"), mdx(\"p\", null, \"The custom return object \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"NrelScraperItem\"), \" is defined in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"items.py\"), \". It expects\\nto field the raw text data, and a string describing the relevant table to push to.\\nThe former is literally ASCII text nested within a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"<body>\"), \" tag, so is easily extracted.\\nIn hindsight, using a simpler tool like \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"\n  }, \"BeautifulSoup\"), \"\\nwould be more than enough to service my needs. But soup is overrated. Fight me.\"), mdx(\"h3\", null, \"Item Pipeline\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"# ./nrel_scraper/pipelines.py\\n\\nfrom itemadapter import ItemAdapter\\nfrom sqlalchemy import create_engine\\nfrom io import StringIO\\nimport pandas as pd\\nimport os\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nclass NrelScraperPipeline(object):\\n\\n    def open_spider(self, spider):\\n\\n        # Retrieve environment variables and keys\\n        hostname = os.getenv('HOSTNAME')\\n        username = os.getenv('USERNAME')\\n        password = os.getenv('PASSWORD')\\n        database = os.getenv('DATABASE')\\n\\n        # Create SQLAlchemy engine\\n        self.engine = create_engine(f'postgresql://{username}:{password}@{hostname}:5432/{database}')\\n\\n    def close_spider(self, spider):\\n        self.engine.dispose()\\n\\n    def process_item(self, item, spider):\\n\\n        # Read ASCII text data into pandas dataframe and process\\n        df = pd.read_csv(StringIO(item['data_text']), sep=\\\",\\\")\\n\\n        # Clean dataframe\\n        df['measurement_ts'] = pd.to_datetime(df['DATE (MM/DD/YYYY)'] + ' ' + df['MST']).dt.tz_localize('MST')\\n        df.drop(['DATE (MM/DD/YYYY)', 'MST'], axis=1, inplace=True)\\n        df['station_id'] = 1\\n        df.columns = df.columns.str.lower()\\\\\\n                               .str.replace(\\\"\\\\(.*\\\\)|\\\\[.*\\\\]\\\", '', regex=True)\\\\\\n                               .str.replace('li-200', 'li200')\\\\\\n                               .str.strip().str.replace('-|\\\\W+', '_', regex=True)\\n\\n        # Write to associated table in Postgres\\n        df.to_sql(item['table'], self.engine, if_exists='append', index=False)\\n\\n        return f\\\"====> Data processed to: {item['table']} table in {self.engine.url.database}\\\"\\n\")), mdx(\"p\", null, \"The extracted return object \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"NrelScaperItem\"), \" must now be transformed and loaded\\ninto our designated database. A couple Python packages are useful here:\\nPandas for data manipulations; SQLAlchemy for interacting with the Postgres instance.\\nThis work is encapsulated in the item pipeline component, \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"NrelScraperPipeline\"), \":\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"open_spider()\"), \": called when the spider is opened, this method retrieves our\\nDB credentials from a gitignored dotenv file and configures an engine to\\nestablish a connection with said Postgres instance.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"process_item()\"), \": called for every item yielded by the spider, this method\\nreads the raw text data into a Pandas dataframe, before cleaning it up and writing\\nit to the desired table in our database.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"close_spider()\"), \": called when the spider is closed to dispose of all connection\\nresources being used by the SQLAlchemy engine.\")), mdx(\"p\", null, \"As you'll notice, the spider itself is operating in tandem with the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"process_item()\"), \"\\nmethod. To activate this pipeline, the class must be added to the scraper's settings\\nand assigned an integer value that determines the order in which it runs (if there\\nwere other activated pipelines too).\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"# ./nrel_scraper/settings.py\\nITEM_PIPELINES = {\\n   'nrel_scraper.pipelines.NrelScraperPipeline': 300,\\n}\\n\")), mdx(\"h2\", null, \"Running the Scraper\"), mdx(\"p\", null, \"With that, this ETL process can be manually triggered with \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"scrapy crawl midc\"), \"\\n(where 'midc' is the spider's name) to upload live data to Amazon RDS.\"), mdx(\"p\", null, \"I'd encourage any first-time users to study Scrapy's \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.scrapy.org/en/latest/topics/architecture.html\"\n  }, \"documentation\"), \",\\nwhere they have some excellent tutorials and provide an overview of the whole\\narchitecture.\"), mdx(\"figure\", null, \"\\n  \", mdx(\"img\", {\n    parentName: \"figure\",\n    \"src\": \"https://docs.scrapy.org/en/latest/_images/scrapy_architecture_02.png\",\n    \"alt\": \"https://docs.scrapy.org/en/latest/topics/architecture.html\"\n  }), \"\\n  \", mdx(\"figcaption\", {\n    parentName: \"figure\"\n  }, \"\\n    A visual overview of Scrapy's architecture. This use-case only touches a few\\n    of the features that make Scrapy a powerful framework. \\n  \")), mdx(\"p\", null, \"As things currently stand, I'd have to wake up at 2AM everyday (i.e. midnight in\\nColorado), run the scrapy crawl for a days-worth of data, and question what I'm\\ndoing with my life.\"), mdx(\"p\", null, \"So, next time I'll revisit this topic and discuss how my ETL pipeline can be\\nfarmed off to a Lambda function using the Serverless Framework - all on AWS Free\\nTier!\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Serverless Web Scraping with AWS, Part I","date":"January 31, 2021","description":"ETL & Analysis of Solar Irradiance Data, Part 1","image":{"childImageSharp":{"resize":{"src":"/static/0c2bad9fa03759685a0d791c14c4bfa3/f3583/nrel_scraper_process.png","height":628,"width":1200}}}}},"previous":{"fields":{"slug":"/clean-energy-innovators/mary-powell/"},"frontmatter":{"title":"Mary Powell - President & CEO, Green Mountain Power"}},"next":{"fields":{"slug":"/nrel/serverless/"},"frontmatter":{"title":"Serverless Web Scraping with AWS, Part II"}}},"pageContext":{"id":"70fdd9c8-02bd-593f-bfe3-22ea657a5303","slug":"/nrel/scraper/","previousPostId":"cd4a0bbf-f5ec-50d5-8d2b-da84cee97b43","nextPostId":"c7509cca-4350-501c-83d2-1a49cf8f5431"}},
    "staticQueryHashes": ["2583974970","3940526654"]}